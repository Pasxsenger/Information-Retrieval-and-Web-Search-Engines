# CSCI 572: Information Retrieval and Web Search Engines

| Project                                                      | Description                                                  | Programming Language                | Tools & Libraries                                            |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ----------------------------------- | ------------------------------------------------------------ |
| [Comparing search engines (Google vs another)](https://github.com/Pasxsenger/Information-Retrieval-and-Web-Search-Engines/blob/main/Comparing%20search%20engines%20(Google%20vs%20another)/README.md) | Compared search results for 100 queries, using Google and DuckDuckGo. | Python                              | BeautifulSoup, HTMLParser, JSON, csv                         |
| [Web-crawling](https://github.com/Pasxsenger/Information-Retrieval-and-Web-Search-Engines/tree/main/Web-crawling) | Work with a web crawler to measure aspects of a crawl, study the characteristics of the crawl, download web pages from the crawl, and gather webpage metadata, all from pre-selected news websites. | Java (crawler); Python (Statistics) | crawler4j; Pandas                                            |
| [Inverted-index creation](https://github.com/Pasxsenger/Information-Retrieval-and-Web-Search-Engines/tree/main/Inverted-index%20creation) | Create Unigram indexes and Bigram indexes using MapReduce and Hadoop | Java                                | MapReduce, Hadoop                                            |
| [LLM-RAG](https://github.com/Pasxsenger/Information-Retrieval-and-Web-Search-Engines/tree/main/LLM-RAG) | Use [Weaviate](https://weaviate.io/), which is a vector DB - stores data as vectors after vectorizing, and computes a search query by vectorizing it and does a similarity search with existing vectors<br /><br />Crawl the web using a Node package, to compile a 'knowledge base' [to use subsequently as input to build a [custom GPT](https://openai.com/blog/introducing-gpts)]<br /><br />Using a Python module, perform RAG (Retrieval-augmented generation) on a 'small', locally-hosted LLM (make that an 'S'LM)<br /><br />use [https://lightning.ai](https://lightning.ai/) to run RAG on their CPU+GPU platform | Python                              | Weaviate, Docker, JSON, Shell, gpt-crawler, pipenv, llama_cpp_python |

